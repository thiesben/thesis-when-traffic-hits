{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing $R_v$\n",
    "The following functions read out the number of observations on a given edge in the road network and the respective speeds for every 15 minute time interval in the time windows 6:00-9:59 and 14:00-19:59."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Imports, options\n",
    "import multiprocessing as mp\n",
    "import numpy as np\n",
    "import os\n",
    "import osmnx as ox\n",
    "import pandas as pd\n",
    "\n",
    "from config import rpath\n",
    "os.chdir(rpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get Waypoint Files\n",
    "path = \"data/Berlin_2017/data/waypoints\"\n",
    "files = []\n",
    "for file in os.listdir(path):\n",
    "    if file.endswith(\"edges.csv\"):\n",
    "        files.append(os.path.join(path,file))\n",
    "files.sort()\n",
    "keepcols = ['TripID', 'WaypointSequence', 'CaptureDate','RawSpeed', 'edge_1', 'edge_2', 'edge_dist']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the current state of the below functions reads the \".._small.csv\" files and does not create them from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_filter_index(file):\n",
    "    '''This function reads a waypoint file, gets the CET timestamp and filters the data frame \n",
    "       to only include files from the peak hours and those who could be sensibly matched with \n",
    "       a road from the street network. It then adds an \"edge_id\" and a \"time_id\" to the data frame,\n",
    "       which are unique edge identifiers and 15-minute time interval identifiers within the\n",
    "       defined time windows, respectively.\n",
    "    '''\n",
    "    # Read file\n",
    "    gdf = pd.read_csv(file)[keepcols] # note to self: dtypes are being read correctly\n",
    "    \n",
    "    # Convert CaptureDate to TimeStamp and convert to CET\n",
    "    gdf[\"Timestamp_CET\"] = pd.to_datetime(gdf[\"CaptureDate\"], format='%Y-%m-%dT%H:%M:%S') \n",
    "    gdf[\"Timestamp_CET\"] = pd.DatetimeIndex(gdf.Timestamp_CET).tz_convert('Europe/Berlin').to_series().tolist() \n",
    "    gdf = gdf.drop(columns=\"CaptureDate\")\n",
    "    \n",
    "    # Get the edge distances to eventually create a histogram or a quantile overview\n",
    "    edge_dist_list = gdf.edge_dist\n",
    "    \n",
    "    # Filter dataframe to only include peak hours during weekdays\n",
    "    gdf = gdf[gdf.Timestamp_CET.dt.weekday < 5] # 5 = Saturday, 6 = Sunday\n",
    "    gdf = gdf[((gdf.Timestamp_CET.dt.hour >= 6) & (gdf.Timestamp_CET.dt.hour < 10)) |\n",
    "              ((gdf.Timestamp_CET.dt.hour >= 14) & (gdf.Timestamp_CET.dt.hour < 20))]\n",
    "    \n",
    "    \n",
    "    # Drop observations that couldn't be matched to a street segment very well\n",
    "    gdf = gdf[gdf[\"edge_dist\"]<=50] \n",
    "    gdf = gdf.drop(columns=\"edge_dist\")\n",
    "    \n",
    "    # Create new unique edge ID column\n",
    "    gdf[\"edge_id\"] = gdf[[\"edge_1\",\"edge_2\"]].astype(str).agg('-'.join, axis=1)\n",
    "    \n",
    "    # Add time identifier\n",
    "    # Create time index for 15 minute blocks\n",
    "    conditions = [\n",
    "        ((gdf.Timestamp_CET.dt.hour == 6) & (gdf.Timestamp_CET.dt.minute < 15)),\n",
    "        ((gdf.Timestamp_CET.dt.hour == 6) & (gdf.Timestamp_CET.dt.minute >= 15) & (gdf.Timestamp_CET.dt.minute < 30)),\n",
    "        ((gdf.Timestamp_CET.dt.hour == 6) & (gdf.Timestamp_CET.dt.minute >= 30) & (gdf.Timestamp_CET.dt.minute < 45)),\n",
    "        ((gdf.Timestamp_CET.dt.hour == 6) & (gdf.Timestamp_CET.dt.minute >= 45)),\n",
    "        ((gdf.Timestamp_CET.dt.hour == 7) & (gdf.Timestamp_CET.dt.minute < 15)),\n",
    "        ((gdf.Timestamp_CET.dt.hour == 7) & (gdf.Timestamp_CET.dt.minute >= 15) & (gdf.Timestamp_CET.dt.minute < 30)),\n",
    "        ((gdf.Timestamp_CET.dt.hour == 7) & (gdf.Timestamp_CET.dt.minute >= 30) & (gdf.Timestamp_CET.dt.minute < 45)),\n",
    "        ((gdf.Timestamp_CET.dt.hour == 7) & (gdf.Timestamp_CET.dt.minute >= 45)),\n",
    "        ((gdf.Timestamp_CET.dt.hour == 8) & (gdf.Timestamp_CET.dt.minute < 15)),\n",
    "        ((gdf.Timestamp_CET.dt.hour == 8) & (gdf.Timestamp_CET.dt.minute >= 15) & (gdf.Timestamp_CET.dt.minute < 30)),\n",
    "        ((gdf.Timestamp_CET.dt.hour == 8) & (gdf.Timestamp_CET.dt.minute >= 30) & (gdf.Timestamp_CET.dt.minute < 45)),\n",
    "        ((gdf.Timestamp_CET.dt.hour == 8) & (gdf.Timestamp_CET.dt.minute >= 45)),\n",
    "        ((gdf.Timestamp_CET.dt.hour == 9) & (gdf.Timestamp_CET.dt.minute < 15)),\n",
    "        ((gdf.Timestamp_CET.dt.hour == 9) & (gdf.Timestamp_CET.dt.minute >= 15) & (gdf.Timestamp_CET.dt.minute < 30)),\n",
    "        ((gdf.Timestamp_CET.dt.hour == 9) & (gdf.Timestamp_CET.dt.minute >= 30) & (gdf.Timestamp_CET.dt.minute < 45)),\n",
    "        ((gdf.Timestamp_CET.dt.hour == 9) & (gdf.Timestamp_CET.dt.minute >= 45)),\n",
    "        ((gdf.Timestamp_CET.dt.hour == 14) & (gdf.Timestamp_CET.dt.minute < 15)),\n",
    "        ((gdf.Timestamp_CET.dt.hour == 14) & (gdf.Timestamp_CET.dt.minute >= 15) & (gdf.Timestamp_CET.dt.minute < 30)),\n",
    "        ((gdf.Timestamp_CET.dt.hour == 14) & (gdf.Timestamp_CET.dt.minute >= 30) & (gdf.Timestamp_CET.dt.minute < 45)),\n",
    "        ((gdf.Timestamp_CET.dt.hour == 14) & (gdf.Timestamp_CET.dt.minute >= 45)),\n",
    "        ((gdf.Timestamp_CET.dt.hour == 15) & (gdf.Timestamp_CET.dt.minute < 15)),\n",
    "        ((gdf.Timestamp_CET.dt.hour == 15) & (gdf.Timestamp_CET.dt.minute >= 15) & (gdf.Timestamp_CET.dt.minute < 30)),\n",
    "        ((gdf.Timestamp_CET.dt.hour == 15) & (gdf.Timestamp_CET.dt.minute >= 30) & (gdf.Timestamp_CET.dt.minute < 45)),\n",
    "        ((gdf.Timestamp_CET.dt.hour == 15) & (gdf.Timestamp_CET.dt.minute >= 45)),\n",
    "        ((gdf.Timestamp_CET.dt.hour == 16) & (gdf.Timestamp_CET.dt.minute < 15)),\n",
    "        ((gdf.Timestamp_CET.dt.hour == 16) & (gdf.Timestamp_CET.dt.minute >= 15) & (gdf.Timestamp_CET.dt.minute < 30)),\n",
    "        ((gdf.Timestamp_CET.dt.hour == 16) & (gdf.Timestamp_CET.dt.minute >= 30) & (gdf.Timestamp_CET.dt.minute < 45)),\n",
    "        ((gdf.Timestamp_CET.dt.hour == 16) & (gdf.Timestamp_CET.dt.minute >= 45)),\n",
    "        ((gdf.Timestamp_CET.dt.hour == 17) & (gdf.Timestamp_CET.dt.minute < 15)),\n",
    "        ((gdf.Timestamp_CET.dt.hour == 17) & (gdf.Timestamp_CET.dt.minute >= 15) & (gdf.Timestamp_CET.dt.minute < 30)),\n",
    "        ((gdf.Timestamp_CET.dt.hour == 17) & (gdf.Timestamp_CET.dt.minute >= 30) & (gdf.Timestamp_CET.dt.minute < 45)),\n",
    "        ((gdf.Timestamp_CET.dt.hour == 17) & (gdf.Timestamp_CET.dt.minute >= 45)),\n",
    "        ((gdf.Timestamp_CET.dt.hour == 18) & (gdf.Timestamp_CET.dt.minute < 15)),\n",
    "        ((gdf.Timestamp_CET.dt.hour == 18) & (gdf.Timestamp_CET.dt.minute >= 15) & (gdf.Timestamp_CET.dt.minute < 30)),\n",
    "        ((gdf.Timestamp_CET.dt.hour == 18) & (gdf.Timestamp_CET.dt.minute >= 30) & (gdf.Timestamp_CET.dt.minute < 45)),\n",
    "        ((gdf.Timestamp_CET.dt.hour == 18) & (gdf.Timestamp_CET.dt.minute >= 45)),\n",
    "        ((gdf.Timestamp_CET.dt.hour == 19) & (gdf.Timestamp_CET.dt.minute < 15)),\n",
    "        ((gdf.Timestamp_CET.dt.hour == 19) & (gdf.Timestamp_CET.dt.minute >= 15) & (gdf.Timestamp_CET.dt.minute < 30)),\n",
    "        ((gdf.Timestamp_CET.dt.hour == 19) & (gdf.Timestamp_CET.dt.minute >= 30) & (gdf.Timestamp_CET.dt.minute < 45)),\n",
    "        ((gdf.Timestamp_CET.dt.hour == 19) & (gdf.Timestamp_CET.dt.minute >= 45)),\n",
    "    ]\n",
    "    values = list(range(len(conditions))) # Corresponding condition values\n",
    "    gdf['time_id'] = np.select(conditions, values) \n",
    "    \n",
    "    return (gdf, edge_dist_list)\n",
    "\n",
    "def get_Rv(file, verbose=True, use_files=True):\n",
    "    '''Reads a file, applies filtering (see read_filter_index), saves the file, and\n",
    "       computes Rv data frame for each segment and time id. Additionally returns a \n",
    "       list of edge_distances.\n",
    "    '''\n",
    "    pathfiles = [path + \"/\" + f for f in os.listdir(path)]\n",
    "    \n",
    "    if use_files:\n",
    "        if file[:-4] + \"_small.csv\" in pathfiles:\n",
    "            gdf = pd.read_csv(file[:-4] + \"_small.csv\")\n",
    "            edge_dist_list = []\n",
    "        else:\n",
    "            print(\"Reading new file...\")\n",
    "            gdf, edge_dist_list = read_filter_index(file)\n",
    "    else:\n",
    "        gdf, edge_dist_list = read_filter_index(file)\n",
    "        \n",
    "    if verbose:\n",
    "        print(\"A file was processed\")\n",
    "    \n",
    "    # Save\n",
    "    if not file[:-4] + \"_small.csv\" in os.listdir(path):\n",
    "        outfile = file[:-4] + \"_small.csv\"\n",
    "        gdf.to_csv(outfile, index=False)\n",
    "        \n",
    "    # Drop unnecessary columns\n",
    "    gdf = gdf.drop(columns=[\"edge_1\", \"edge_2\"])\n",
    "    \n",
    "    # Return Rv df\n",
    "    return gdf[['edge_id', 'time_id', 'RawSpeed']].groupby(['edge_id', 'time_id']).agg(['count', 'sum']), edge_dist_list\n",
    "\n",
    "def get_and_process_Rv(files, verbose=True):\n",
    "    ''' Gets Rv for every file, processes it to get a clean Rv table for every edge and quarter of an hour.\n",
    "        Prints out quantiles of edge distances and returns clean Rv data frame.\n",
    "    '''\n",
    "    # Start the multiprocessing pool and get Rv\n",
    "    pool = mp.Pool(processes=min(len(files), 7))\n",
    "    out = pool.map(get_Rv, files)\n",
    "    pool.close()\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Pool closed\")\n",
    "\n",
    "    Rv_list = [x[0] for x in out]\n",
    "    edge_dist_list_list = [x[1] for x in out]\n",
    "    \n",
    "    del out # for space\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Begin concatenating\")\n",
    "        \n",
    "    Rv = pd.concat(Rv_list).groupby(['edge_id', 'time_id']).agg(['sum'])\n",
    "    \n",
    "    del Rv_list # for space\n",
    "    \n",
    "    # Remove entries that have n = 0 (only nan Speeds)\n",
    "    Rv = Rv[Rv.xs(('RawSpeed','count', 'sum'), axis=1) != 0]\n",
    "    \n",
    "    # Remove obs with n < median n (20) on selected edge_id\n",
    "    Rv = Rv[Rv.xs(('RawSpeed','count', 'sum'), axis=1) >= np.median(Rv.xs((\"RawSpeed\", \"count\", \"sum\"), axis=1))]\n",
    "    print(\"The median count of observations on an edge is\", np.median(Rv.xs((\"RawSpeed\", \"count\", \"sum\"), axis=1)))\n",
    "    # Add meanSpeed column\n",
    "    Rv.loc[:,\"meanSpeed\"] = Rv.xs(('RawSpeed','sum', 'sum'), axis=1)/Rv.xs(('RawSpeed','count', 'sum'), axis=1)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Begin merging lists\")\n",
    "        \n",
    "    edge_dists = [item for sublist in edge_dist_list_list for item in sublist]\n",
    "    edge_dists = np.array(edge_dists)\n",
    "    print(\"# all waypoints: \" + str(len(edge_dists)))\n",
    "    print(\"# waypoints with dist >50: \" + str(len(edge_dists[edge_dists<=50])))\n",
    "    \n",
    "    return Rv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A file was processed\n",
      "A file was processed\n",
      "A file was processed\n",
      "A file was processedA file was processed\n",
      "\n",
      "A file was processed\n",
      "A file was processed\n"
     ]
    }
   ],
   "source": [
    "Rv = get_and_process_Rv(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have the mean speeds for each road segment and time. We save this for further usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Rv.to_csv(\"data/Rv.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ma2]",
   "language": "python",
   "name": "conda-env-ma2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
